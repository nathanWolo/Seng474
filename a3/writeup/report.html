<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>SENG 474: Assignment 3 Report</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="report_files/libs/clipboard/clipboard.min.js"></script>
<script src="report_files/libs/quarto-html/quarto.js"></script>
<script src="report_files/libs/quarto-html/popper.min.js"></script>
<script src="report_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="report_files/libs/quarto-html/anchor.min.js"></script>
<link href="report_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="report_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="report_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="report_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="report_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">SENG 474: Assignment 3 Report</h1>
<p class="subtitle lead">Nathan Woloshyn</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Clustering algorithms are unsupervised machine learning techniques that aim to partition a dataset into groups or clusters, based on the similarity between data points. These algorithms are widely used in various domains, such as pattern recognition, image processing, market segmentation, and anomaly detection.</p>
<p>In this assignment, we will explore and analyze the performance of four clustering methods on two provided datasets. The clustering methods include two variants of Lloyd’s algorithm (k-means) with different initialization strategies, and two variants of hierarchical agglomerative clustering with different linkage criteria.</p>
<p>Specifically, we will implement Lloyd’s algorithm with uniform random initialization and k-means++ initialization. For hierarchical agglomerative clustering, we will use Euclidean distance as the dissimilarity measure and employ both single linkage and average linkage as clustering criteria. Our goal is to compare the performance of these methods and gain insights into the characteristics of the datasets and the clustering results.</p>
</section>
<section id="section-1-lloyds-algorithm-methods" class="level1">
<h1>Section 1: Lloyd’s Algorithm Methods</h1>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<p>The implementation of the clustering methods is divided into two main sections: Lloyd’s algorithm with uniform random initialization and k-means++ initialization. In both cases, Python and the NumPy library were used to perform the required calculations efficiently.</p>
<section id="lloyds-algorithm-with-uniform-random-initialization" class="level3">
<h3 class="anchored" data-anchor-id="lloyds-algorithm-with-uniform-random-initialization">Lloyd’s Algorithm with Uniform Random Initialization</h3>
<p>The first step in the implementation of the random initialization method was to define a function that calculates the Euclidean distance between two points in n-dimensional space. The <code>euclidean_distance</code> function was created for this purpose, using NumPy’s <code>linalg.norm</code> function.</p>
<p>Next, the <code>random_initialization</code> function was implemented to provide a uniform random initialization strategy for the cluster centroids. This function selects k centroids uniformly at random from the dataset.</p>
<p>Finally, the primary function, <code>kmeans_clustering</code>, was implemented to run the k-means algorithm on the given data using the specified initialization method. The algorithm iteratively assigns each data point to the nearest centroid, updates the centroids based on the mean of the assigned points, and checks for convergence using a tolerance value. If the algorithm does not converge within the maximum number of iterations (for this assignment, 100 was chosen as the maximum), it prints a warning message.</p>
</section>
<section id="lloyds-algorithm-with-k-means-initialization" class="level3">
<h3 class="anchored" data-anchor-id="lloyds-algorithm-with-k-means-initialization">Lloyd’s Algorithm with k-means++ Initialization</h3>
<p>For the k-means++ initialization, the <code>kmeans_plus_plus_initialization</code> function was implemented. This function selects the first centroid uniformly at random from the dataset and then selects the remaining centroids based on their squared distances to the closest existing centroid. The probabilities of choosing a data point as the next centroid are proportional to these squared distances.</p>
<p>The <code>kmeans_clustering</code> function was also used to run the k-means algorithm with the k-means++ initialization method.</p>
</section>
</section>
<section id="experiments-and-visualization" class="level2">
<h2 class="anchored" data-anchor-id="experiments-and-visualization">Experiments and Visualization</h2>
<p>To evaluate the performance of the clustering algorithms, a series of experiments were run on two datasets using different values of k. The <code>run_kmeans_experiment</code> function was implemented to run the k-means clustering algorithm with different values of k and initialization methods on a given dataset. The clustering results were then visualized using 2D or 3D scatter plots generated by the <code>plot_clusters</code> function, where each cluster is represented by a different color and centroid markers are represented by black ‘x’ symbols.</p>
<p>The cost of the clustering solutions, calculated as the sum of squared Euclidean distances between data points and their assigned centroids, was plotted against the number of clusters (k) using the <code>plot_cost_vs_k</code> function. This function creates a line plot to compare the costs of k-means clustering with random initialization and k-means++ initialization for different values of k. Additionally, the <code>plot_iterations_vs_k</code> function was implemented to visualize the number of iterations required for convergence as a function of k for both initialization methods.</p>
<p>Below are some example clusterings, as well as the cost and number of iterations plots, for the two datasets.</p>
<section id="dataset-1" class="level3">
<h3 class="anchored" data-anchor-id="dataset-1">Dataset 1</h3>
<section id="clusterings" class="level4">
<h4 class="anchored" data-anchor-id="clusterings">Clusterings</h4>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><img src="figs/clusters_dataset1_k_2_k-means++.png" class="img-fluid" style="width:40.0%" alt="Dataset 1 Clusterings-1"> <img src="figs/clusters_dataset1_k_3_random.png" class="img-fluid" style="width:40.0%" alt="Dataset 1 Clusterings-2"> <img src="figs/clusters_dataset1_k_7_k-means++.png" class="img-fluid" style="width:40.0%" alt="Dataset 1 Clusterings-3"> <img src="figs/clusters_dataset1_k_7_random.png" class="img-fluid" style="width:40.0%" alt="Dataset 1 Clusterings-4"></p>
</div>
</div>
</div>
<p>As can be observed in the two k=7 figures, the two initialization methods produce very similar clusterings. This similarity in performance is expected, as the data points being clustered are randomly generated and both methods are capable of effectively partitioning the dataset.</p>
</section>
<section id="cost-vs.-k" class="level4">
<h4 class="anchored" data-anchor-id="cost-vs.-k">Cost vs.&nbsp;k</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs/cost_vs_k_dataset1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Dataset 1 Cost vs.&nbsp;k</figcaption><p></p>
</figure>
</div>
<p>The cost vs.&nbsp;k plot illustrates the relationship between the number of clusters (k) and the sum of squared Euclidean distances between data points and their assigned centroids. As the value of k increases, the cost generally decreases due to the data points being more closely grouped around their respective centroids. However, the rate of decrease tapers off as k becomes larger, indicating diminishing returns for increasing k. Both initialization methods, random and k-means++, show the same trends in cost reduction, suggesting that their performance in partitioning Dataset 1 is comparable. This observation further supports the idea that both methods are effective at clustering the randomly generated data points in this dataset. Both methods are plotted here, but because their cost values are so similar it is difficult to distinguish the two lines.</p>
</section>
<section id="iterations-vs.-k" class="level4">
<h4 class="anchored" data-anchor-id="iterations-vs.-k">Iterations vs.&nbsp;k</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs/iterations_vs_k_dataset1.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Dataset 1 Iterations vs.&nbsp;k</figcaption><p></p>
</figure>
</div>
<p>The iterations vs.&nbsp;k plot reveals the number of iterations required for the k-means algorithm to converge for different values of k and initialization methods. The relationship appears noisy, but a noticeable trend emerges where k-means++ initialization generally converges faster than random initialization, particularly at higher values of k. Furthermore, k-means++ never reaches the iteration limit of 100, while random initialization does so twice. This result indicates that the k-means++ initialization method has an advantage in terms of convergence speed, especially when using larger values of k. Faster convergence can lead to more efficient clustering, highlighting the benefits of using the k-means++ initialization method over random initialization.</p>
</section>
</section>
<section id="dataset-2" class="level3">
<h3 class="anchored" data-anchor-id="dataset-2">Dataset 2</h3>
<section id="clusterings-1" class="level4">
<h4 class="anchored" data-anchor-id="clusterings-1">Clusterings</h4>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><img src="figs/clusters_dataset2_k_3_k-means++.png" class="img-fluid" style="width:40.0%" alt="Dataset 2 Clusterings-1"> <img src="figs/clusters_dataset2_k_3_random.png" class="img-fluid" style="width:40.0%" alt="Dataset 2 Clusterings-2"> <img src="figs/clusters_dataset2_k_25_k-means++.png" class="img-fluid" style="width:40.0%" alt="Dataset 2 Clusterings-3"> <img src="figs/clusters_dataset2_k_25_random.png" class="img-fluid" style="width:40.0%" alt="Dataset 2 Clusterings-4"></p>
</div>
</div>
</div>
<p>Similar to Dataset 1, the clusterings generated by both initialization methods appear quite alike in the k=3 scenario. However, when k is increased to 25, a discernible difference emerges between the resulting clusterings of the two techniques. This discrepancy is likely attributable to the greater underlying structure present in the data, which allows the k-means++ initialization method to consistently position centroids in a manner distinct from random initialization.</p>
</section>
<section id="cost-vs.-k-1" class="level4">
<h4 class="anchored" data-anchor-id="cost-vs.-k-1">Cost vs.&nbsp;k</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs/cost_vs_k_dataset2.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Dataset 2 Cost vs.&nbsp;k</figcaption><p></p>
</figure>
</div>
<p>Despite the visible differences in the clusterings produced by the two initialization methods, it is noteworthy that they achieve similar cost values. This outcome highlights the effectiveness of both approaches in minimizing the sum of squared Euclidean distances between data points and their corresponding centroids. As a result, even though the cluster assignments may appear different, the overall quality of the clustering solutions remains comparable, demonstrating that both random and k-means++ initialization methods are capable of providing efficient partitioning of the dataset.</p>
</section>
<section id="iterations-vs.-k-1" class="level4">
<h4 class="anchored" data-anchor-id="iterations-vs.-k-1">Iterations vs.&nbsp;k</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs/iterations_vs_k_dataset.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Dataset 2 Iterations vs.&nbsp;k</figcaption><p></p>
</figure>
</div>
<p>Interestingly, in this particular dataset, random initialization tends to converge faster than k-means++ for most k values, except for the maximum k (25). One possible explanation for this counterintuitive finding could be the “tube” shape of the data. This unique structure might lead to a situation where random initialization occasionally places centroids closer to the optimal positions by chance, whereas k-means++ might struggle to effectively capture the elongated nature of the data distribution during its centroid seeding process. Consequently, random initialization could require fewer iterations to converge, as it starts from a position that is closer to the final solution. However, this is a noisy process, so the number of iterations required for convergence can vary significantly. Due to time limitations, the experiment was only run ten times for each k value, so the results are not conclusive. Further experimentation would be required to confirm this hypothesis.</p>
</section>
</section>
</section>
</section>
<section id="section-2-hierarchical-agglomerative-clustering" class="level1">
<h1>Section 2: Hierarchical Agglomerative Clustering</h1>
<p>The primary goal of these experiments is to investigate the performance and behavior of HAC using single and average linkage methods, and evaluate the clustering results with varying distance thresholds.</p>
<section id="experimental-setup" class="level2">
<h2 class="anchored" data-anchor-id="experimental-setup">Experimental Setup:</h2>
<p>To assess the behavior of the hierarchical agglomerative clustering algorithm, a set of experiments were conducted on two datasets using single and average linkage methods and varying distance thresholds. The <code>run_hac</code> function was implemented to run HAC with specified linkage methods and thresholds on a given dataset. The clustering results were visualized using 2D or 3D scatter plots generated by the <code>plot_hac_clusters</code> function, with each cluster represented by a different color.</p>
<p>Dendrograms were generated using the <code>plot_dendrogram</code> function to display the hierarchical structure of clusters and the distance between clusters at different levels of the hierarchy. These plots were created for each combination of dataset, linkage method, and threshold.</p>
<p>By analyzing the generated plots and dendrograms, we can gain insights into how the choice of linkage method and distance threshold influences the clustering results, and determine the optimal threshold value for each linkage method. The experiments provide valuable information on the performance of HAC using single and average linkage methods on two different datasets, and facilitate the comparison of their effectiveness in producing meaningful clusters.</p>
</section>
<section id="experimental-results" class="level2">
<h2 class="anchored" data-anchor-id="experimental-results">Experimental Results</h2>
<section id="dataset-1-1" class="level3">
<h3 class="anchored" data-anchor-id="dataset-1-1">Dataset 1</h3>
<section id="sample-clustering-results" class="level4">
<h4 class="anchored" data-anchor-id="sample-clustering-results">Sample Clustering Results</h4>
<section id="single-linkage" class="level5">
<h5 class="anchored" data-anchor-id="single-linkage">Single Linkage</h5>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs\hac_single_dataset1_threshold0.01.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Dataset 1 Single Linkage Clustering-1</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs\hac_single_dataset1_threshold0.3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Dataset 1 Single Linkage Clustering-2</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="average-linkage" class="level5">
<h5 class="anchored" data-anchor-id="average-linkage">Average Linkage</h5>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs\hac_average_dataset1_threshold1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Dataset 1 Average Linkage Clustering-1</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs\hac_average_dataset1_threshold0.6.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Dataset 1 Average Linkage Clustering-2</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="dendrograms-truncated-to-15" class="level4">
<h4 class="anchored" data-anchor-id="dendrograms-truncated-to-15">Dendrograms (truncated to 15)</h4>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs\dendrogram_single_dataset1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Dataset 1 Single Linkage Dendrogram</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs\dendrogram_average_dataset1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Dataset 1 Average Linkage Dendrogram</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="dataset-2-1" class="level3">
<h3 class="anchored" data-anchor-id="dataset-2-1">Dataset 2</h3>
<section id="sample-clustering-results-1" class="level4">
<h4 class="anchored" data-anchor-id="sample-clustering-results-1">Sample Clustering Results</h4>
<section id="single-linkage-1" class="level5">
<h5 class="anchored" data-anchor-id="single-linkage-1">Single Linkage</h5>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs/hac_single_dataset2_threshold1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Dataset 2 Single Linkage Clustering-1</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs/hac_single_dataset2_threshold0.05.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Dataset 2 Single Linkage Clustering-2</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="average-linkage-1" class="level5">
<h5 class="anchored" data-anchor-id="average-linkage-1">Average Linkage</h5>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs/hac_average_dataset2_threshold0.7.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Dataset 2 Average Linkage Clustering-1</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs/hac_average_dataset2_threshold1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Dataset 2 Average Linkage Clustering-1</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="dendrograms-truncated-to-15-1" class="level4">
<h4 class="anchored" data-anchor-id="dendrograms-truncated-to-15-1">Dendrograms (truncated to 15)</h4>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs/dendrogram_single_dataset2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Dataset 2 Single Linkage Dendrogram</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figs/dendrogram_average_dataset2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Dataset 2 Average Linkage Dendrogram</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<section id="dataset-1-2" class="level3">
<h3 class="anchored" data-anchor-id="dataset-1-2">Dataset 1</h3>
<p>In the case of Dataset 1, the single linkage method produced less coherent clusters compared to the k-means clustering results. The single linkage method is sensitive to noise and outliers, which can lead to the formation of elongated or chain-like clusters. This behavior is evident in the sample clustering results for single linkage, where we observe less distinct and less compact clusters.</p>
<p>On the other hand, the average linkage method generated more coherent and well-defined clusters for Dataset 1. Average linkage tends to create more balanced and compact clusters, as it considers the average distance between pairs of points from different clusters. This results in a more robust clustering solution, which is less influenced by noise and outliers. The sample clustering results for average linkage demonstrate that this method was better suited for Dataset 1, producing more meaningful and visually appealing clusters, similar to those obtained using k-means clustering.</p>
<p>In addition to the clustering results, we also examined the dendrograms for Dataset 1 to determine the most meaningful split by identifying the longest join. For the single linkage method, cutting the dendrogram at a distance of 0.3 resulted in the inclusion of all visible clusters on the dendrogram. While this approach allowed for a more detailed view of the cluster structure, it did not yield particularly coherent clusters, as previously discussed.</p>
<p>For the average linkage method, cutting the dendrogram at a distance of 5 produced a partition that divided the data into two well-defined clusters. This cut was chosen based on the observation that it corresponded to the longest join, and thus represented a significant separation between the two resulting clusters. The average linkage method’s ability to form more balanced and compact clusters is evident in this partition, further supporting the conclusion that average linkage is better suited for clustering Dataset 1.</p>
</section>
<section id="dataset-2-2" class="level3">
<h3 class="anchored" data-anchor-id="dataset-2-2">Dataset 2</h3>
<p>For Dataset 2, both single and average linkage methods failed to produce visually appealing or sensible clusters. This outcome can be partially attributed to the nature of the dataset, which does not exhibit the two obvious groupings that were present in Dataset 1. Consequently, the HAC algorithm struggles to identify meaningful and distinct clusters in the data.</p>
<p>It is important to note that the k-means clustering methods also generated rather unappealing clusters for this dataset, indicating that the inherent structure of the data might not be well-suited for the clustering techniques employed in these experiments. The lack of clearly separable groups in Dataset 2 may contribute to the difficulty in obtaining meaningful clustering results.</p>
<p>Given the performance of both HAC and k-means clustering algorithms on Dataset 2, it may be beneficial to explore alternative methods in order to better capture the underlying structure of the data.</p>
<p>In order to further analyze the clustering results for Dataset 2, we also examined the dendrograms generated for both single and average linkage methods. We opted to cut both dendrograms low, including all visible clusters, with a distance of 0.4 for single linkage and a distance of 6 for average linkage. By making these cuts, we aimed to capture the most significant separations between clusters, while still considering the visible structure of the dendrograms.</p>
<p>However, even with these cuts, the resulting clusters did not yield particularly meaningful or visually appealing results. This observation further emphasizes the challenges in clustering Dataset 2 using the HAC algorithm with single and average linkage methods. The absence of clearly separable groups in the dataset and the sensitivity of the linkage methods to noise may contribute to the suboptimal clustering results.</p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>